<!DOCTYPE html>

<html lang="en">

<head>

<meta charset="utf-8">

<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>JACR - A Reactive Bearing Angle Only Obstacle Avoidance Technique for Unmanned Ground Vehicles</title>
<link href='http://fonts.googleapis.com/css?family=PT+Sans:300,400,400italic,500,600,700,700italic&amp;subset=latin,greek-ext,cyrillic,latin-ext,greek,cyrillic-ext,vietnamese' rel='stylesheet' type='text/css' />

<link href='http://fonts.googleapis.com/css?family=Antic+Slab:300,400,400italic,500,600,700,700italic&amp;subset=latin,greek-ext,cyrillic,latin-ext,greek,cyrillic-ext,vietnamese' rel='stylesheet' type='text/css' />

<link rel="stylesheet" href="../css/style.css" />

<link href="../css/bootstrap.css" rel="stylesheet">
<link href="../images/icon.ico" rel="shortcut icon" type="image/x-icon">
<!--Goole Analytics Most Visited Article Code-->

<script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-47353717-1']);
 _gaq.push(['_trackPageview']);

 (function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
 })();

</script>


<!-- Include Most Visited Articles JS Code -->
 <script type="text/javascript" src="mostvisited.js"></script> 

<!-- Include SHARE THIS js sources -->
<script type="text/javascript">var switchTo5x=true;</script>
<script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
<script type="text/javascript">stLight.options({publisher: "8f49e037-5155-4595-abc0-d069deae6488", doNotHash: true, doNotCopy: false, hashAddressBar: false, onhover:false});</script>
</head>

<body>

<div id="wrapper">

 <div class="header">

 <div class="container">

 <div class="row">

 <header id="header">
  <div class="logo col-sm-4"><a href="http://avestia.com"><img src="/images/logo.png" alt="Journal" class="img" /></a></div>
  <div class="threemenu col-sm-5">
  <div class="row"><a class="link col-sm-4" href="http://avestia.com"><img src="/images/home.jpg" class="img"></a> <a class="link col-sm-4" href="http://avestia.com/journals/"><img src="/images/journals.jpg" class="img"></a> <a class="link col-sm-4" href="http://amss.avestia.com"><img src="/images/submit.jpg" class="img"></a></div>
  </div>
  <div class="searching col-sm-3">
<div class="sharethis">Share this page:<br/>
<span class='st_email'></span>
<span class='st_facebook'></span>
<span class='st_twitter'></span>
<span class='st_googleplus'></span>
<span class='st_linkedin'></span>
</div>
  
  <div class="search_wrapper">
  <form class="search" action="../results" method="get">
  <fieldset>
  <span class="text">
  <input name="q" id="q" type="text" value="" placeholder="Search..." /><button class="search_top_button">GO</button>
  </span>

  </fieldset>
  </form>
  
  </div>
  </div>
  <div class="col-lg-12">

  <div class="avada-row">

  <nav id="nav" class="nav-holder">

  <div role="navigation" class="navbar navbar-default">

  <div class="container-fluid">

   <div class="navbar-header">
   
   <button data-target=".navbar-collapse" data-toggle="collapse" class="navbar-toggle" type="button"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
<span class="navbar-brand" href="#">Menu</span>
   </div>

   <div class="navbar-collapse collapse">

   <ul class="nav navbar-nav">

					<li><a href="/">Journal Home</a></li><span class="menu-line">|</span>
   <li><a href="../aims/">Aims and Scopes</a></li><span class="menu-line">|</span>
   <li><a href="../charges/">Article Processing Charges</a></li><span class="menu-line">|</span>
   <li><a href="../guidelines/">Author Guidelines</a></li><span class="menu-line">|</span>
   <li><a href="../board/">Editorial Board</a></li><br>
   <li><a href="../volumes/">Past Volumes</a></li><span class="menu-line">|</span>
   <li><a href="../current/">Current Volume</a></li><span class="menu-line">|</span>
   <li><a href="../propose/">Propose a Special Topic</a></li><span class="menu-line">|</span>
   <li><a href="../register/">Receive Updates</a></li><span class="menu-line">|</span>
   <li><a href="../contact/">Contact Us</a></li>



   </ul>

   </div>

   <!--/.nav-collapse -->

  </div>

  <!--/.container-fluid -->

  </div>

  </nav>

  </div>

  </div>
  <div class="clearfix"></div>

 </header>

 </div>

 </div>
<div class="clearfix"></div>
 </div>

 <div class="slider">

 <div class="container">

 <div class="row"><div class="col-lg-12"><img class="img2" src="../images/header.png" alt="img"></div></div>

 </div>

 </div>

<div class="container">
 <div class="clearfix"></div>
 <div class="row">

 <div class="col-lg-12 text">
 
<div align=center>
<div class="table">

 <table class=MsoTableGrid border=0 cellspacing=0 cellpadding=0 style='border:none; width: 100%'>
  <tr>
  <td>Volume 1, Year 2014 - Pages 22-28</td>
  <td align="right"><a href="PDF/004.pdf">View PDF (Full-text)</a></td>
 </tr>
  <tr>
  <td>DOI: 10.11159/jacr.2014.004</td>
  <td align="right"><a href="#references">Linked References</a></td>
 </tr>
 <tr>
		<td colspan="2">ISSN: 2368-6677</td>
		</tr>
 </table>
 
</div>
</div>

<br>

<p align="center" style="font-size:22pt;">A Reactive Bearing Angle Only Obstacle Avoidance Technique for Unmanned Ground Vehicles</p>

<p align="center">

 <strong>Jonathan Lwowski, Liang Sun, Roberto Mexquitic-Saavedra, Rajnikant Sharma, Daniel Pack</strong>
 
 <br>Department of Electrical and Computer Engineering, The University of Texas at San Antonio<br>
One UTSA Circle, San Antonio, United States of America<br>
kyx930@my.utsa.edu; liang.sun@utsa.edu; irn901@my.utsa.edu;<br>
rajnikant.sharma@utsa.edu; Daniel.Pack@utsa.edu
 
</p>
 

<p align="justify"><i><strong>Abstract- </strong>
In this paper, we present a reactive, vision based obstacle avoidance technique for Unmanned Ground Vehicles (UGVs) navigating with only visual electro-optic sensors in an indoor environment without Global Positioning System (GPS) signals. The presented work is unique in that, to the best of our knowledge, there was no other bearing angle only obstacle avoidance technique for ground vehicles reported in the literature. A Lyapunov-based sliding mode controller maintains the bearing angle of the UGV from obstacles, using a real-time image processing method. The proposed technique is implemented and tested with a Pioneer robot in an indoor environment, and the results demonstrate its effectiveness.</i></p>

<p align="left"><i><strong>Keywords:</strong></i> Reactive obstacle avoidance, Bearing angle only navigation technique, Unmanned systems, Sliding mode control.</p>

<p>&nbsp;</p>
<p align="justify"> &copy; Copyright 2014 Authors - This is an Open Access article published under the <a href="http://creativecommons.org/licenses/by/3.0" target="_blank">Creative Commons Attribution License terms</a>. Unrestricted use, distribution, and reproduction in any medium are permitted, provided the original work is properly cited.</p>

<p>&nbsp;</p>

<p>
 Date Received: 2014-05-29 <br />
 
 Date Accepted: 2014-10-16 <br />
 
 Date Published: 2014-10-20
</p>

<img src="../images/line.png" width="100%;" height="1" />
<h2>1. Introduction</h2>

<p align="justify" style="text-indent:19.85pt;">Over the past decade, an increasing number of unmanned systems have been used in both military and civilian applications such as surveillance, survey, and search and rescue missions. One of the enabling technologies of unmanned mobile systems is the ability to autonomously plan paths to navigate in an environment while avoiding obstacles (Goldman, 1994). While global path planning approaches are used to guide a UGV to a destination, a local path planning is necessary for a UGV to reactively avoid unexpected obstacles while following an overall trajectory generated by a global path planner.</p>

<p align="justify" style="text-indent:19.85pt;">Huang et al. (2006) presented a reactive obstacle avoidance method using a steering potential function. In their work,
they seek to find a path with the lowest probability of a collision with an obstacle, but the method is heavily dependent on extensive parameter calibration beforehand. Minguez (2005) and Fulgenzi et al. (2007) used a laser range sensor to detect obstacles. In the latter work, they introduced the Probabilistic Velocity Obstacle (PVO) method, which estimates the probably of a collision with an obstacle and computes the optimal obstacle avoiding path for the UGV in a dynamic occupancy grid. Bandyophadyay et al. (2010) used a short range 2-D laser sensor to allow Autonomous Surface Craft (ASC) to reactively avoid
obstacles. They used simple linear prediction based on the current history of obstacles and ASC dynamics to determine the best path for USV to take. Saunders and Beard (2008) proposed a reactive method for an unmanned aerial vehicle (UAV) to avoid local obstacles. The method requires an estimation of the range between a UAV and an obstacle along with computing the bearing angle. Sharma et al. (2012) applied a sliding mode controller using only the bearing angle for a UAV to avoid local obstacles in simulation, on which the current work is based. Lensar and Veloso (2003) presented an obstacle avoidance strategy using images from a single camera to estimate the range and angle to an obstacle with a known color. Michels et al. (2005) developed a monocular vision based method to avoid obstacles, incorporating a learning algorithm. The algorithm was trained using cameras labeled with ground-truth distances to the closest obstacle. Lagisettey et al. (2013) used a stereo camera system to detect static obstacles by using a stereo matching algorithm combined with a triangulation method.</p>

<p align="justify" style="text-indent:19.85pt;">To the best of our knowledge, the method presented in this paper is the first reported reactive bearing angle only
obstacle avoiding controller for UGVs. The controller maintains the bearing angle of a UGV to an obstacle without the need for any range measurements nor any parameter calibrations. Conventional image processing methods were employed to obtain the bearing angle between a UGV and obstacles. A real-time Lyapunov-based sliding mode controller allows a UGV to reactively avoid unexpected obstacles by controlling the angular velocity of the UGV.</p>

<p align="justify" style="text-indent:19.85pt;">The rest of the paper is organized as follows. Section II presents the image processing technique used by a UGV to find the bearing angle between itself and an obstacle in a local coordinate frame. The proposed obstacle avoidance algorithm is described in Section III, followed by Section IV where we present experimental results. Concluding remarks are presented in the final section.</p>

<h2>2. Extracting Obstacle Information</h2>

<p align="justify" style="text-indent:19.85pt;">In this section, we describe the image processing algorithms used to compute the bearing angle between an obstacle and a UGV. The raw image is first smoothed using the following Gaussian operator </p>

<div class="eqn"><img src="004_files/image001.png" class="img"></div>
<div class="eqn-number">(1)</div><br>

<p align="justify" style="text-indent:19.85pt;">where <img src="004_files/image002.png" class="img"> and <img src="004_files/image003.png"class"img">are pixel coordinates, and &#963; is the standard deviation of the Gaussian distribution function (Reinhard, 2010). The image is then converted from the red, blue and green (RGB) color space to the hue, saturation and value (HSV) color space, which contains additional information such as luma, color intensity, and chroma. Next, the image is thresholded based on a color (red is selected for our experiments). Figure 1 shows the original and processed images, respectively. </p>

<!-- put into a table? -->

<figure><img src="004_files/image004.jpg" class="img"><img src="004_files/image005.jpg" class="img">
<figcaption>Figure 1. Two images of an obstacle: (a) raw image and (b) processed image</figcaption>
</figure>
<br>

<p align="justify" style="text-indent:19.85pt;">Once an image has gone through the conversion and the thresholding process, the moments of the resulting image are calculated by </p>

<div class="eqn"><img src="004_files/image006.png" class="img"></div><div class="eqn-number">(2)</div>
<br>

<p align="justify" style="text-indent:19.85pt;">where <img src="004_files/image007.png"> and <img src="004_files/image008.png" class="img">denotes the pixel intensity at pixel location<img src="004_files/image009.png">(Flusser and Suk, 2006) (Web-1). Next, the centroid of the moment, i.e., the center of obstacle mass, was calculated by</p>
<div class="eqn">
<img src="004_files/image010.png" class="img"></div>
<div class="eqn-number">(3)</div>
<br>

<p align="justify" style="text-indent:19.85pt;">Finally, the bearing angle, <img src="004_files/image011.png" class="img">, as shown in Figure 3(1), can be computed by</p>
<div class="eqn"><img src="004_files/image012.png" class="img"></div> <div class="eqn-number">(4)</div>
<br>
<p align="justify" style="text-indent:19.85pt;">where <img src="004_files/image013.png" class="img"> is the focal length of a camera used. We hastily add that the image processing techniques we used are simple and that sophisticated methods exist in the literature. The simple techniques are used since the focus of the paper is the development of the bearing angle only controller.</p>

<figure><img border=0 id="Picture 4" src="004_files/image014.png" class="img">
<figcaption>Figure 2. Block diagram of feedback control</figcaption>
</figure>
<br>

<h2>3. Bearing Angle Only Based Obstacle Avoidance Technique</h2>

<p align="justify" style="text-indent:19.85pt;">In this section, we describe the UGV controller, shown in Figure, made of two sub-controllers. The first sub-controller controls the UGV when no obstacle is detected, while the second sub-controller is the reactive obstacle avoidance unit activated when obstacles are detected. The first module generates simple straight line trajectories to move from one location to another. The second sub-controller, the focus of this paper, is constructed based on a Lyapunov-based sliding mode control algorithm.</p>

<figure><img border=0 id="Picture 1" src="004_files/image015.png" class="img"/>
<figure><img border=0 id="Picture 2" src="004_files/image016.png" class="img"/>
<figcaption>(a) The entire obstacle is in the camera field of view. </figcaption><figcaption>(b) Only a part of an obstacle is in the camera field of view.</figcaption>
</figure>
</figure>
<br>
<figcaption>Figure 3. Top-down views of a UGV approaching a cylindrical obstacle.</figcaption>
<br>

<p align="justify" style="text-indent:19.85pt;">As shown in Figure 3, letting <img src="004_files/image017.png" class="img">and <img src="004_files/image018.png" class="img">be the range and bearing angle from a UGV to the center of the obstacle, respectively, <img src="004_files/image019.png" class="img">and&nbsp;<img src="004_files/image020.png" class="img">be the range and bearing angle from a UGV to the nearest visible edge of the obstacle, respectively, and<img src="004_files/image021.png" class="img"> be the constant linear velocity of the UGV, the equations of motion of the system are given by (Saunders and Beard, 2008).</p>

<img src="004_files/image022.png" class="img"><br>
<img src="004_files/image023.png" class="img">

<img src="004_files/image024.png" class="img"><br>
<img src="004_files/image025.png" class="img">

<p align="justify" style="text-indent:19.85pt;">where <img src="004_files/image026.png" class="img">is the heading angle of the UGV.</p>

<p align="justify" style="text-indent:19.85pt;">In this paper, we propose two additional variables,&nbsp;
	<img src="004_files/image027.png" class="img">and 
	<img src="004_files/image028.png" class="img">, defined as the range and bearing angle to the center of the visible obstacle in the image, as shown in Figure 3(b). If the entire obstacle is captured by the camera, we have&nbsp;
	<img src="004_files/image029.png" class="img">and 
 <img src="004_files/image030.png" class="img">. As a controller drives the UGV such that the obstacle is pushed to the edge of the camera view, the portion of an obstacle captured by the camera image shrinks, implying that
 <img src="004_files/image031.png" class="img">and 
 <img src="004_files/image032.png" class="img">. The objective is to develop a control law for <img src="004_files/image033.png" class="img">, which is able to effectively regulate the UGV such that the image of the obstacle is moved appropriately to the edge of the field of view (FOV) of the camera. Inspired by Sharma et al. (2012), we developed the following sliding mode controller to make&nbsp;
 <img src="004_files/image028.png" class="img">converge to a specified angle <img src="004_files/image034.png" class="img">without the range information.</p>

<div class="eqn"><img src="004_files/image035.png" class="img"></div>
<div class="eqn-number">(5)</div>

<br>
<br>

<div class="eqn"><img src="004_files/image036.png" class="img"></div>
<div class="eqn-number">(6)</div>

<br>
<br>

<p align="justify" style="text-indent:19.85pt;">where <img src="004_files/image037.png" class="img"> is a positive constant representing the minimum of <img src="004_files/image027.png" class="img">, <img src="004_files/image038.png" class="img"> is a small constant preventing <img src="004_files/image039.png" class="img"> from being stuck at 0 when&nbsp; <img src="004_files/image028.png" class="img"> is zero, <img src="004_files/image040.png" class="img"> is the slope constant of a saturation function, <img src="004_files/image041.png" class="img"> denotes a saturation function and <img src="004_files/image042.png" class="img"> represents the sign of the argument.</p>

<p align="justify" style="text-indent:19.85pt;">The controller operates in the following manner. When an obstacle is captured by the camera, it assigns <img src="004_files/image043.png" class="img"> as the camera FOV, and s elects <img src="004_files/image044.png" class="img">, i.e., <img src="004_files/image045.png" class="img">, where <img src="004_files/image046.png" class="img"> is a small positive constant which represents the portion of the obstacle captured by the camera. When the entire obstacle is captured, <img src="004_files/image029.png" class="img"> and <img src="004_files/image030.png" class="img">, it is easily shown that the controller (5) requires minimal changes in vehicle heading compared to the ones generated by the controller proposed by Sharma et al. (2012), generating a shorter UGV path.</p>

<p align="justify" style="text-indent:19.85pt;">As the captured portion of an obstacle decreases, <img src="004_files/image028.png" class="img"> approaches <img src="004_files/image020.png" class="img">. From Figure 2(b) we can obtain </p><div class="eqn"><img src="004_files/image047.png" class="img"></div>

<br>

<div class="eqn"><img src="004_files/image048.png" class="img"></div>
<br>

<p align="justify" style="text-indent:19.85pt;">Thus, when <img src="004_files/image049.png" class="img"> we have <img src="004_files/image050.png" class="img"></p>

<p align="justify" style="text-indent:19.85pt;">Then <img src="004_files/image051.png" class="img">, similar to the equations derived by Saunders and Beard (2008).</p>

<figure>
<img border=0 id="Picture 3" src="004_files/image052.png" class="img">
<figcaption>Figure 4. UGV kinematics during a curve motion.</figcaption>
</figure>
<br>

<p align="justify" style="text-indent:19.85pt;">For a differential-drive ground robot, the next task is to convert <img src="004_files/image033.png" class="img"> to the corresponding linear velocities of the left and right wheels. Consider a differential-drive robot is moving along a curve with a constant velocity <img src="004_files/image021.png" class="img"> and a turning rate <img src="004_files/image033.png" class="img">, as shown in Figure 4, where ICC stands for the Instantaneous Center of Curvature, <img src="004_files/image053.png" class="img">is the instantaneous radius of the curvature, and <img src="004_files/image054.png" class="img"> is the distance between the two wheels. Assuming that there is no side-slip on the wheels, the angular velocities of the robot center and the left and right wheels relative to the ICC should be the same, which also equals to the turning rate of the UGV. Letting <img src="004_files/image055.png" class="img"> and <img src="004_files/image055.png" class="img"> be the linear velocities of the left and right wheels, respectively, we have <img src="004_files/image056.png" class="img">
Thus, the linear velocities of the two wheels of the UGV can be calculated by </p>

<div class="eqn"><img src="004_files/image057.png" class="img"></div>
<div class="eqn-number">(7)</div>
<br>
<img src="004_files/image058.png" class="img">
<h2 align="left">4. Experimental Results</h2>

<p align="justify" style="text-indent:19.85pt;">In this section, we present sample experimental results with corresponding analysis. The experiments were conducted in an indoor laboratory with normal office lighting. As shown in Figure 5, A Pioneer P3-DX ground robot, a Logitech-C615 camera with a field of view of approximately 74 degrees and a red cylinder (dimensions stated in Table 1) were used as the UGV, the visual sensor, and the obstacle, respectively. Table 1 shows the dimensions of the units we used in our experiments.</p>

<div class="widetable">
<table style="border: thin solid black;">
<caption>Table 1. Dimensions of UGV, Computer, Camera and Obstacle</caption>

<div align="center">

 <tr>
 <td style="padding:5px;">
 	<p align="justify" style="text-indent:19.85pt;">&nbsp;</p>
 </td>
 <td style="padding:5px">
	Width
 (m)
 </td>
 <td style="padding:5px">
 Length
 (m)
 </td>
 <td style="padding:5px">
 Height
 (m)
 </td>
 </tr>
 <tr>
 <td style="padding:5px">
 UGV
 </td>
 <td style="padding:5px">
 0.381
 </td>
 <td style="padding:5px">
 0.455
 </td>
 <td style="padding:5px">
 0.237
 </td>
 </tr>
 <tr>
 <td style="padding:5px">
 Computer
 </td>
 <td style="padding:5px">
 0.387
 </td>
 <td style="padding:5px">
 0.254
 </td>
 <td style="padding:5px">
 0.038
 </td>
 </tr>
 <tr>
 <td style="padding:5px">
 Camera
 </td>
 <td style="padding:5px">
 0.066
 </td>
 <td style="padding:5px">
 0.101
 </td>
 <td style="padding:5px">
 0.012
 </td>
 </tr>
 <tr>
 <td style="padding:5px">
 Obstacle
 </td>
 <td style="padding:5px">
 0.19
 </td>
 <td style="padding:5px">
 0.19
 </td>
 <td style="padding:5px">
 0.546
 </td>
 </tr>
</tbody>
</table>
</div>
<br>

<figure>
<img id="Picture 5" src="004_files/image059.jpg" class="img">
<figcaption>Figure 5. Pioneer P3DX UGV and an obstacle</figcaption>
</figure>
<br>

<p align="justify" style="text-indent:19.85pt;">A Dell M4600 laptop computer (Intel Core i7 2.2GHz Quad Core 8GB RAM) onboard the Pioneer robot processed camera images and generated control inputs for the UGV. On average, the controller issued a new command every 0.33 seconds, processing a 620 x 480 color image and performing 44 multiplication or division operations, 21 addition or subtraction operations and 11 trigonometric functions. For all experiments, we assume that obstacles consist of a single distinctive color. Scenarios of multiple obstacles are also discussed later in this section. For our experiments, the robot assumes that its starting location is (0, 0) in a global coordinate frame. The nominal linear velocity of the robot, V, is a selected as 0.07 m/s&nbsp;.&nbsp;.&nbsp;Focal length, <img src="004_files/image060.png" class="img">, <img src="004_files/image038.png" class="img">, <img src="004_files/image061.png" class="img">, <img src="004_files/image034.png" class="img">were selected as .424 m, 5 m, .1, .3 and 35 degrees, respectively. Figures 6 and 7 show scenarios in which the UGV was trying to reach its destination at (0, 2.7) m with obstacles at (0.3, 1.8) m and (-0.3, 1.8) m, respectively. In Figure 8, the UGV was attempting to reach a destination at (-1.2, 2.7) m with an obstacle located at (-0.6, 1.2) m. Figure 9 shows a similar setup as in Figure 8, but with a destination of (1.2, 2.7) m and the obstacle is located at (0.6, 1.2) meters.</p>
 
<figure><img id="Picture 7" src="004_files/image062.jpg" class="img">
<figcaption>Figure 6. UGV responding to an obstacle to its right. </figcaption>
</figure>
<br>

<figure><img id="Picture 8" src="004_files/image063.jpg" class="img">
<figcaption>Figure 7. UGV responding to an obstacle to its left.</figcaption>
</figure>
<br>

<figure><img id="Picture 9" src="004_files/image064.jpg" class="img">
<figcaption>Figure 8. UGV maneuvering to the left with an obstacle directly in its path.</figcaption>
</figure>
<br>

<figure><img id="Picture 10" src="004_files/image065.jpg" class="img">
<figcaption>Figure 9. UGV maneuvering to the right with an obstacle directly in its path.</figcaption>
</figure>
<br>

<p align="justify" style="text-indent:19.85pt;">Experiments with multiple obstacles were also conducted. Figures 10 and 11 show the resulting UGV paths when multiple obstacles are detected on the UGV&#39;s way to a destination. If multiple obstacles are detected simultaneously by the camera in the path of the UGV, as in Figure 10, the image processing algorithm proposed in Section II considers the area expanded by those obstacles as a single large obstacle. Therefore, even though the space between obstacles may be large enough for the UGV to pass through, the UGV will not consider it as an option. In Figure 11, the first obstacle was placed at (0.3, 0.9) m. The second obstacle was dropped at (-0.1, 3.1) m in real-time only after the UGV successfully avoided the first obstacle. The resulting path shows that the UGV is able to reactively avoid unexpected obstacles and still reach its destination. From the results shown in Figures 6-10, it can be seen empirically that the proposed controller successfully commands the UGV to reach its destination without any collisions with obstacles.</p>

<figure><img id="Picture 11" src="004_files/image066.jpg" class="img">
<figcaption>Figure 10. UGV maneuvering straight with 2 obstacles seen simultaneously.</figcaption>
</figure>
<br>

<figure><img id="Picture 12" src="004_files/image067.jpg" class="img">
<figcaption>Figure 11. UGV reactively maneuvering with two pop-up obstacles.</figcaption>
</figure>
<br>

<h2 align="left">5. Conclusion</h2>

<p align="justify" style="text-indent:19.85pt;">This paper presents one of the first bearing-angle-only-based nonlinear controllers for mobile robots to reactively avoid obstacles in a GPS denied environment. Experimental results demonstrate the validity of the proposed controller. In future work, we plan to integrate the bearing-angle-only controller with a simultaneous localization and mapping (SLAM) algorithm in order to obtain better estimates of the UGV's locations in a global coordinate system. We also plan on improving the image processing technique to isolate individual obstacles and to expand the types of obstacles it can detect.</p>

<h2 align="left">Acknowledgments </h2>

<p align="justify" style="text-indent:19.85pt;">The research was supported by the McNair Scholars Program. The authors acknowledge the contributions of Negar Farmani, Azima Mottaghi, and Rajdeep Dutta for their constructive feedback on the earlier version of the controller and the experimental results.</p>


<br>
<H2 align="left"><a name="references">References</a></H2>

<p align="justify">[1] Bandyophadyay T., Sarcione L., Hover F. (2010). "A Simple Reactive Obstacle Avoidance Algorithm and Its Application in Singapore Harbor." Field and Service Robotics, 455 &#8212; 465. <a href="http://dx.doi.org/10.1007/978-3-642-13408-1_41" target="_blank">View Article</a></p>

<p align="justify">[2] Flusser J., Suk T. (2006) Rotation Moment Invariants for Recognition of Symmetric Objects, Image Processing, 3784 &#8212; 3790. <a href="http://library.utia.cas.cz/separaty/historie/flusser-rotation%20moment%20invariants%20for%20recognition%20of%20symmetric%20objects.pdf" target="_blank">View Article</a></p>

<p align="justify">[3] Fulgenzi C., Spalanzani A., Laugier C. (2007). Dynamic Obstacle Avoidance in uncertain environment combining PVOs and Occupancy Grid, IEEE International Conference on Robotics and Automation, 1610 &#8212; 1616. <a href="http://dx.doi.org/10.1109/ROBOT.2007.363554" target="_blank">View Article</a></p>

<p align="justify">[4] Goldman J. (1994). Path Planning Problems and Solutions. Aerospace and Electronics Conference, 105 &#8212; 108. <a href="http://dx.doi.org/10.1109/NAECON.1994.333013" target="_blank">View Article</a></p>

<p align="justify">[5] Huang J., Fajen B., Fink J., Warren W. (2006) Visual navigation and obstacle avoidance using a steering potential function. Robotics and Autonomous Systems Conference, 288 &#8212; 289. <a href "http://dx.doi.org/10.1016/j.robot.2005.11.004" target="_blank">View Article</a></p>

<p align="justify">[6] Lagisetty R., Phillip N. K., Padhi R., Bhat M. S. (2013). Object Detection and Obstacle Avoidance for Mobile Robot using Stereo Camera, IEEE International Conference on Control Applications, S. 886 &#8212; 891. <a href="http://dx.doi.org/10.1109/CCA.2013.6662816" target="_blank">View Article</a></p>

<p align="justify">[7] Lenser S., Veloso M. (2003). Visual Sonar: Fast Obstacle Avoidance Using Monocular Vision, IEEE International Conference on Intelligent Robots and Systems, 605 &#8212; 610 <a href="http://dx.doi.org/10.1109/IROS.2003.1250741" target="_blank">View Article</a></p>


<p align="justify">[8] Michels J., Saxena A., Y. Ng. A. (2005), High Speed Obstacle Avoidance using Monocular Vision and Reinforcement Learning, International Conference on Machine Learning. <a href="http://dx.doi.org/10.1145/1102351.1102426" target="_blank">View Article</a></p>

<p align="justify">[9] Minguez J. (2005). Integration of Planning and Reactive Obstacle Avoidance in Autonomous Sensor-Based Navigation. Intelligent Robots and Systems, 2486 &#8212; 2492. <a href="http://dx.doi.org/10.1109/IROS.2005.1544993" target="_blank">View Article</a></p>

<p align="justify">[10] Reinhard E. (2010). High dynamic range imaging: Acquisition, Display, and Image-Based Lighting. <a href="http://books.google.ca/books?id=w1i_1kejoYcC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false" target="_blank">View Book</a></p>

<p align="justify">[11] Saunders J., Beard R. (2008). Reactive Vision Based Obstacle Avoidance with Camera Field of View
Constraints. AIAA Guidance, Navigation and Control Conference and Exhibit, 2008 &#8212; 7250. <a href="http://arc.aiaa.org/doi/abs/10.2514/6.2008-7250" target="_blank">View Article</a></p>

<p align="justify">[12] Sharma R., Saunders J., Beard W. (2012). Reactive Path Planning for Micro Air Vehicles Using Bearing Only Measurements. Journal of Intelligent and Robotic System, 65, 409 &#8212; 416. <a href="http://dx.doi.org/10.1007/s10846-011-9617-x" target="_blank">View Article</a></p>

<p align="justify">Web Sites:</p>

<p align="justify">[13] Web-1: <a href="http://www.homepages.inf.ed.ac.uk/rbf/CVonline/l">http://www.homepages.inf.ed.ac.uk/rbf/CVonline/l</a> consulted 20 June. 2013.</p>

<div class="clearfix"></div>
 </div>
<div class="clearfix"></div>
 </div>
<div class="clearfix"></div>
 </div>

 </div>

 <div class="footer">

 <div class="container">

 <div class="col-sm-7 footer-link">

 <p><a href="/">Avestia Publishing</a></p>
 <p><a href="../register/">Subscribe to our Mailing List</a></p>
 <p><script>var refURL = window.location.protocol + "//" + window.location.host + window.location.pathname; document.write('<a href="http://avestia.com/feedback/?refURL=' + refURL+'">Feedback</a>');</script></p>
 <p><a href="../terms/">Terms of Use</a></p>
		<p><a href="../sitemap/">Sitemap</a></p>

 </div>
 <div class="col-sm-5 footer-txt">
 <p>Avestia Publishing, International ASET Inc. </p>
 <p>Unit No. 417, 1376 Bank St. </p>
 <p>Ottawa, Ontario, Canada </p>
 <p>Postal Code: K1H 7Y3</p>
 <p>Phone Number: 1-613-695-3040</p>

 </div>

 </div>

 </div>

</div>
<script src="../js/jquery.js"></script>
<script src="../js/bootstrap.js"></script>
</body>

</html>