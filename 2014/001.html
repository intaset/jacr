<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive">
<meta name="description" content="The Volume 1 published for JACR Journal.">
<meta name="keywords" content="avestia, publishing, journals, papers, science, renewable energy">
<title>JACR -  Alternative Vision Approach to Ground Vehicle Detection System Utilizing Single Board Computer (SBC) for Motor Control</title>

<meta name="handheldfriendly" content="true">
<meta name="mobileoptimized" content="240">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> 
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link href="../css/avestia.css" rel="stylesheet">
<link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic|Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
<link rel="shortcut icon" href="../img/icon.ico" type="image/x-icon">
<!--[if IE-9]><html lang="en" class="ie9"><![endif]-->

<script src="../js/modernizr.custom.63321.js"></script>
<script type="text/javascript" src="../mostvisited.js"></script> 
<script type="text/javascript" src="../mostvisitedExt.js"></script> 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68628727-1', 'auto');
  ga('send', 'pageview');

</script>
<script>
  (function() {
    var cx = '016656741306535874023:1cg0j788xos';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
</head>
<body class="loading">
<nav id="slide-menu">
  <h1>Avestia Publishing</h1>
  <ul>
    <li><a href="http://avestia.com/about">About Us</a></li>
    <li><a href="http://avestia.com/ethics">Ethics in Publishing</a></li>
    <li><a href="http://avestia.com/openaccess">Open Access</a></li>
    <li><a href="http://avestia.com/editor">Become a Reviewer or an Editor</a></li>
    <li><a href="http://avestia.com/publishing">Your Publishing Needs</a></li>
    <li><a href="http://avestia.com/proceedings">Conference Proceedings</a></li>
    <li><a href="http://avestia.com/news">Latest News</a></li>
    <li><a href="http://avestia.com/guidelines">Author Guidelines</a></li>
    <li><a href="http://avestia.com/journals">Journals</a></li>
    <li><a href="http://amss.avestia.com/">Submission</a></li>
    <li><a href="http://avestia.com/copyright">Copyright</a></li>
    <li><a href="http://avestia.com/contact">Contact Us</a></li>
  </ul>
</nav>

<div id="content">
  <div class="desktop">
<div class="cbp-af-header">
  <div class="cbp-af-inner grid">
    <div class="unit unit-s-2-8 unit-m-2-8 unit-l-1-7">
      <a href="http://avestia.com"><img src="../img/logo.svg" class="flex-logo" alt="Avestia Publishing"></a>
    </div>

    <div class="unit unit-s-1-2 unit-m-4-0 unit-l-5-1">
    <div class="nav1">
      <nav>
        <a href="http://avestia.com">Home</a>
        <a href="http://amss.avestia.com/">Submission</a>
        <a href="http://avestia.com/journals">Journals</a>
        <a href="http://avestia.com/ethics">Ethics in Publishing</a>
        <a href="http://avestia.com/guidelines">Author Guidelines</a>
      </nav>
    </div>
    </div>

    <div class="unit unit-s-3-4 unit-m-3-5 unit-l-1-6 unit-l-3-2">
    <div class="search-menu">
      <div class="menu-trigger-1"><p class="menu">MENU</p></div><br>
      <gcse:searchbox-only resultsUrl="../results"></gcse:searchbox-only>
    </div>
    </div>

    <div class="unit unit-s-1 unit-m-1 unit-l-1">
    <div class="nav">
      <nav>
        <a href="http://avestia.com">Home</a>
        <a href="http://amss.avestia.com/">Submission</a>
        <a href="http://avestia.com/journals">Journals</a>
        <a href="http://avestia.com/ethics">Ethics in Publishing</a>
        <a href="http://avestia.com/guidelines">Author Guidelines</a>
      </nav>
    </div>
    </div>
  </div>
</div>
  </div>

  <header>
    <div class="mobile">
      <div class="cbp-af-header">
  <div class="cbp-af-inner">
    <div class="unit unit-s-3-4 unit-m-1-3 unit-l-1-3">
          <a href="http://avestia.com"><img src="../img/logo.svg" class="flex-logo" alt="Avestia Publishing"></a>
      </div>
      <div class="unit unit-s-1-3 unit-m-2-3 unit-m-2-3-1 unit-l-2-3">
          <div class="menu-trigger"></div>
      </div>
  </div>
</div>
      <div class="bg">
        <gcse:searchbox-only resultsUrl="../results"></gcse:searchbox-only>
      </div>
    </div> <!--Mobile -->
  </header>

  <div class="j-header-article">
  <div class="name">
    <h1>Journal of Automation and Control Research (JACR)<br>
    <p class="body">ISSN: 2368-6677</p></h1>
    <div class="oalink">
    <a href="http://avestia.com/openaccess" target="blank" title="Avestia's Open Access">
          <img src="../img/j-oa.png" border="0" onmouseover="this.src='../img/j-oa-hover.png'" onmouseout="this.src='../img/j-oa.png'" class="j-oa">
    </a>
  </div>
  </div>
</div>

  <div role="navigation" class="navbar navbar-default">
  <ul>
    <li><a href="../">Journal Home</a></li>
    <li><a href="../aims">Aims & Scopes</a></li>
    <li><a href="../fee">Publishing Fee</a></li>
    <li><a href="../board">Editorial Board</a></li>
    <li><button data-target=".navbar-collapse" data-toggle="collapse" class="navbar-toggle" type="button">Volumes</button></li>
    <li><a href="../contact">Contact Us</a></li>
  </ul>
  <div class="navbar-collapse collapse">
    <ul class="nav navbar-nav">
      <li><a href="../volume1">Volume 1</a></li>
      <li><a href="../current">Current Volume</a></li>
    </ul>
  </div><!--/.nav-collapse -->
</div>

<div class="grid">
<div class="unit unit-s-1 unit-m-1 unit-l-1">
  <div class="main-content j-home">
    <div class="unit unit-s-1 unit-s-1-2 unit-m-1-2 unit-l-1-2">
      <p class="body">Volume 1, Year 2014 - Pages 1-10<br>
      DOI: 10.11159/jacr.2014.001</p>
    </div>

    <div class="unit unit-s-1 unit-s-1-2 unit-m-1-2 unit-l-1-2 a-link">
        <a href="PDF/001.pdf" class="body-link" class="body-link">View PDF (Full-text)</a><br>
      <a href="#references" class="body-link">Linked References</a>
    </div>

    <h3 class="center">Alternative Vision Approach to Ground Vehicle Detection System Utilizing Single Board Computer (SBC) for Motor Control</h3>

    <p class="body-bold center">Nurul Izzati Mohd. Saleh, Md. Farhan Aizuddin, Wan Rahiman</p>

    <p class="body center">Universiti Sains Malaysia Engineering Campus, School of Electrical & Electronic Engineering<br>
    Seberang Perai Selatan, Nibong Tebal, Pulau Pinang, 14300, Malaysia<br>
    nims14_eee041@student.usm.my; michael3a10@gmail.com; wanrahiman@usm.my</p>

    <p class="body"><b>Abstract</b> - <i>This paper presented an alternative approach for ground vehicle identification for vehicle following. The vision system is tested for its feasibility in image processing on a limited resources platform. The limited resources platform consists of off-the-shelf webcam as vision sensor, single board computer (SBC) as its main hardware and a microcontroller as a sensor board. The image processing library used for image processing is OpenCV, an open source library. For optimization purposes Design of Experiment (DOE) is used to determine the factors that contribute to the accuracy of the vehicle identification. This system is then attached serially to ultrasonic sensor in order to demonstrate the safety system for the follower vehicle. The experimental results reveal that the approach under limited resources platform is able to identify the needed features to indicate presence of a ground vehicle to some extent. The paper also highlights the limitations of the system which may be addressed in future works.</i></p>

    <p class="body"><b><i>Keywords:</i></b> Ground vehicle; Autonomous vehicle; Vision system; Median filter; OpenCV.</p>

    <p class="body">© Copyright 2015 Authors This is an Open Access article published under the <a href="http://creativecommons.org/licenses/by/3.0" class="body-link" target="_blank" class="body-link">Creative Commons Attribution License terms</a>. Unrestricted use, distribution, and reproduction in any medium are permitted, provided the original work is properly cited.</p>

    <p class="body">Date Received: 2015-05-24<br>
    Date Accepted: 2015-11-09<br >
    Date Published: TBA</p>

  <div class="border"></div>

  <div class="indent">
  <h4>1. Introduction</h4>

  <p class="body">Road injuries and fatalities are a growing concern in Malaysia. There has been an increase in the number of deaths due to road accidents from 6,286 deaths in 2003 to 6,917 in 2012. From the fatality distribution by mode of transport chart, fatalities caused by car as the mode of transport made up 22 % from the overall chosen modes [1].</p>
  <p class="body">In urban driving environment, accidents or collisions especially in traffic congestion scenario usually occur due to human error such as misjudged distance, loss of control, poor manoeuvring and sudden braking. Drivers sometimes misjudge the distance between their cars and the vehicles in front of them during traffic jams such as rush hour or road congestion due to road works, resulting in collisions when the vehicles in front apply the brakes without warning. Therefore, there is a need for a robust vision system to allow ground vehicles to detect the presence of vehicle in front of them.</p>
  <p class="body">This paper focuses on urban driving scenario where accidents
involving motorized vehicles often happen during traffic congestion for
example, during peak hours. A webcam-based approach to identify whether a
ground vehicle is present in front of the autonomous vehicle is proposed in
this study.</p>

<h4>2. Related Work</h4>

<p class="body">Existing works in the research and development of visual guidance
technology can be categorized into two major categories: Unmanned Ground
Vehicles (UGVs) and Intelligent Transport Systems (ITSs) [2]. UGVs study deal with off-road navigation and terrain mapping [3]. ITS study on the other hand, is
concentrated more on efficient transport in structured or urban settings. There
are several roles that can be assigned to the vision system which are detection
and following of a road [4], detection of obstacles ,
detection and tracking of other vehicles and detection and identification of
landmarks [5].</p>

<p class="subhead">2.1. Imaging System</p>
<p class="body">There are two types of imaging system, active and passive system. The related review discussed will be focusing on the latter system. In order to
obtain good quality data input to guidance algorithm there are several considerations to be taken in choosing
the cameras for passive imaging in outdoor environments [2].</p>
<p class="body">One of the common image
acquisition methods that can be used is through passive camera system [4]. Camera peripheral are often
chosen due to their availability, and the resolution can be varied to suit the
needs. With multiple inputs of images from different sides of the vehicle
obtained from more than one camera, an algorithm was designed by computing a
maximum value of every row for every field and the average of these maximum
values of the five fields was then used as the threshold value of each row
after integral compensation. The formulae are generated after a few repetitive
experiments. This method is reported by [4] to be lacking in terms of its robustness
towards different types of conditions and illumination.</p>
<p class="body">Other method of image acquisition that can be utilized is by using
stereovision camera [3], [5], [6]. Stereovision-based distance
measurement provides reasonably good accuracy for objects within a short
distant range. However, due to the reported matching ambiguity, quantization
errors and inaccurate parameters of the camera model from various sources by [6], this method has poor accuracy
for objects at long distance. Thus further processing is needed in order to
compensate for these shortcomings.</p>

<p class="subhead">2. 2. Image Processing</p>
<p class="body">Morphological operation in image processing context is the means
for obtaining image components that are functional in characterizing and
describing the shape region such as boundaries, skeleton and convex hull [7]. Morphological operations are
usually performed on binary images. There are two morphological operations that
are considered which are dilation and erosion.</p>
<p class="body">Dilation is a morphological operation that thickens or expands the
white region. Dilation is useful when the region of interest (ROI) is too small
for further processing or when there are broken parts that need to be joined as
an object. Erosion on the other hand is a morphological operation that removes
excessive white pixels. Erosion is useful for removing small white noises or
separating two connected regions [8].</p>
<p class="body">In order to remove smaller detected regions usually due to noise,
and to enlarge the areas of object of interests, sequence of erode and dilate
operation are involved in the morphological operation where the effect is, and
to close any holes within them.</p>

<p class="subhead">2. 3. Feature Extraction</p>
<p class="body">One of the feature extraction methods covered in this literature
review is by using Scale Invariant Feature Transform (SIFT) [5]. SIFT is a method for extracting
distinctive invariant features from images that can be used to perform reliable
matching between different views of an object or scene. However, to implement
this method, it requires a lot of processing resources. To reduce the SIFT
computational time, Self-Organizing Map (SOM) is used to improve the matching process [5].</p>
<p class="body">There are also other key-point descriptors that are available such
as BRISK, SURF and BRIEF.</p>
<p class="body">Another method is to detect
the outer contours or edge of the object of interest [3]. For image processing, the
algorithm can be designed by importing the relevant libraries from OpenCV.
OpenCV (Open Source Computer Vision Library) is a computer vision and machine
learning software library. It is developed as an open source and used as an
alternative to MATLAB, and serves to provide foundation for computer vision
purposes and function aid of the use of machine perception in the commercial
products [9].</p>
<p class="body">To date, OpenCV provides the library for the feature extraction
method discussed (SIFT, SURF, BRIEF and BRISK); to name a few.</p>
<p class="body">For this project, the
vision system utilizes passive monocular camera which only focuses on image
processing.</p>

<h4>3. Methodology</h4>
<p class="body">The proposed method provides an alternative approach from the
related works reviewed. While there are a lot of state-of-the-art method that
can be utilized this method is proposed as a solution to be implemented to a
single board computer (SBC) as it is simple and computationally inexpensive in
comparison to the other method. Figure 1 shows the proposed system block
diagram. The system will be installed on a Remote Controlled (RC) car which
serves as the prototype for an autonomous vehicle. The installed webcam will be
streaming continuously and the video feed will be fed into SBC to process with
the aid of OpenCV libraries.</p>
<p class="body">Figure 2 shows the overall flowchart for the image processing. The
image processing is performed fully with the aid of OpenCV built-in libraries.</p>

<figure>
<img src="001_files/image001.png" class="article-img">

<figcaption>
Figure 1.
System Block Diagram.</figcaption>
</figure>

<figure>
<img src="001_files/image2.png" class="article-img">

<figcaption>
Figure 2.
Image Processing Flowchart.</figcaption>
</figure>

<p class="subhead">3. 1. Image Acquisition</p>
<p class="body">The webcam chosen has CMOS type sensor with 2 mm focal length. The
chosen webcam has flexible hinge so that the elevation angle could be adjusted.
The image is acquired form the video streaming feed and fed directly into
microprocessor.</p>

<p class="subhead">3. 2. Image Segmentation</p>
<p class="body">The initial RGB colour space input from the webcam was converted
to HSV colour space. The camera would look for pixel with the same Hue (H),
Saturation (S) and Value (V) range in order to detect the rear view of the lead
vehicle. The region of interest (ROI) was converted to white whereas the rest was
converted to black. The outer corner of the contour was for subsequent operation.
Figure 3 shows the segmented image.</p>

<figure>
<img src="001_files/image003.png" class="article-img">

<figcaption>
Figure 3.
Segmented image.</figcaption>
</figure>

<p class="subhead">3. 3. Image Filtering</p>
<p class="body">In order to reduce noise, a median filter was used. The median
filter in OpenCV library uses Eq. 1 and replaces value in a square (represented
by matrix value) neighbourhood around the centre pixel with its corresponding
median value. Figure 4 shows the result of applying median filter to the
segmented image.</p>

<div class="equation">
<img src="001_files/image004.png" class="eqn">
<div class="eqn-number">(1)</div>
</div>

<p class="body">Since the processing was done in real time the lighting (though
barely noticeable) changed frequently. Simple blurring by using average filter
was not as effective since it was highly affected by noisy images. Large
difference in pixel points could cause a noticeable movement in the average
value. It might cause the region of the noise to expand. Median filtering
however was able to ignore the outliers by selecting the middle points [10].</p>

<figure>
<img src="001_files/image4.png" class="article-img">

<figcaption>
Figure 4.
Segmented image after filter operation.</figcaption>
</figure>

<p class="subhead">3. 4. Morphological Operation</p>
<p class="body">This operation was performed to correct the imperfections caused by
the segmented images. Closing was the result of dilating the image followed by
erosion. Fig. 5 shows the subsequent closing morphological operation done on
the image.</p>

<figure>
<img src="001_files/image5.png" class="article-img">

<figcaption>
Figure
5. Segmented image after consequent morphological operation.</figcaption>
</figure>

<p class="subhead">3. 5. Feature Extraction</p>
<p class="body">From the extracted contour information, bounding box can be
traced. The visual representation is shown in Figure 6 to show characteristic
of the bounding box whereas Figure 7 shows the final result. This function draws a rectangle box that bounds the contour. If
there are several regions that have been segmented, the algorithm is designed
to look out for the maximum width and height of the contour and draw out the
bounding box. The formula used to
determine the maximum bounding box is depicted in Eq. 2 – 3 where <img src="001_files/image007.png"> is the width of
the detected contour(s) and <img src="001_files/image008.png"> is the height
of the contour(s). Meanwhile, <img src="001_files/image009.png"> and <img src="001_files/image010.png"> is the starting point of the detected edge of the
contour(s) whereas <img src="001_files/image011.png"> and <img src="001_files/image012.png"> is the end
point of the detected edge of the contour(s) alongside the x-axis and y-axis.</p>

<figure>
<img src="001_files/image6.png" class="article-img">

<figcaption>
Figure
6. Visual representation of bounding box.</figcaption>
</figure>

<div class="equation">
<img src="001_files/image014.png" class="eqn">
<div class="eqn-number">(2)</div>
</div>

<div class="equation">
<img src="001_files/image015.png" class="eqn">
<div class="eqn-number">(3)</div>
</div>

<p class="body">In this case, the
resolution of the camera had been set, therefore the pixel limit for width and
height depicted by <img src="001_files/image016.png"> and <img src="001_files/image017.png"> is shown in Eq.
4.</p>

<div class="equation">
<img src="001_files/image018.png" class="eqn">
<div class="eqn-number">(4)</div>
</div>

<figure>
<img src="001_files/image7.png" class="article-img">

<figcaption>
Figure
7. Final result.</figcaption>
</figure>

<p class="subhead">3. 7. Sensor Integration</p>
<p class="body">In order to gauge the distance between lead vehicle and follower
vehicle sensor is used. The sensor used is ultrasonic sensor. This sensor will
be integrated to the system.</p>
<p class="body">Once the vehicle is detected, SBC will prompt sensor board to gauge the
distance. A microcontroller is used to get values from ultrasonic sensor and
convert it into readable entity to be relayed back to SBC serially. SBC will
decide the action of the motor of the RC car through Case 1, 2 and
3(represented in equation (5)).</p>

<div class="equation">
<img src="001_files/image020.png" class="eqn">
<div class="eqn-number">(5)</div>
</div>

<p class="body">Figure 8 shows the flowchart for the decision making for motor control.
If the distance is too far (case 1) the follower vehicle will accelerate. If
the distance is too near (case 2) the follower vehicle will stop. If the
distance is ideal (case 3) the follower vehicle will maintain its speed.</p>

<p class="subhead">3. 8. Optimization using Design of Experiment (DOE)</p>
<p class="body">In order to determine the factors that contribute to the accuracy of
the vehicle detection, an experiment was performed. The experiment utilized 2<sup>3</sup> factorial experimental methods. This method is useful as it provides the
smallest number of trial runs (in this
case 2<sup>3</sup> = 8 trials) [11]. The factors that were tested are; camera
elevation angles (the degree of the way camera is positioned), environment
lighting conditions, and the distance between the lead vehicle and the
following vehicle. For each factor there are 2 levels, arbitrarily indicated as
low (-1) and high (1). Table 1 shows the selection factors and its respective
levels. These levels were selected based on the initial pilot test that had
been performed. These levels however, are not robust to other kind of webcam as
the levels were tested exclusively based on the webcam’s specification. The
trials were tested on a completely randomized order with a total of 3
replicates. This was done to ensure that each trial has an equal chance of
being affected by extraneous factors. Extraneous factor may affect the
responses of the trials between replicates and within replicates.</p>

<figure>
<img src="001_files/image8.png" class="article-img">

<figcaption>
Figure 8. Motor Control Decision Making.</figcaption>
</figure>

<center>
<p class="body">Table 1. Selection of Factors and Levels.</p>
<div class="widetable">
<table>
<thead>
 <tr>
  <td>Factors</td>
  <td colspan="2">Levels</td>
 </tr>
 <tr>
  <td>-1</td>
  <td>1</td>
 </tr>
</thead>
<tbody>
 <tr>
  <td>Factor A: Camera elevation angle</td>
  <td>20°</td>
  <td>10°</td>
 </tr>
 <tr>
  <td>Factor B: Lighting</td>
  <td>Dim</td>
  <td>Bright</td>
 </tr>
 <tr>
  <td>Factor C: Distance</td>
  <td>0.5m</td>
  <td>1.0m</td>
 </tr>
</tbody>
</table>
</div>
</center>

<p class="body">The design matrix for
the 2<sup>3 </sup>factorial adapted from [11] is shown in Table 2.</p>

<center>
<p class="body">Table 2. Design Matrix.</p>
<div class="widetable">
<table>
<thead>
<tr>
  <td>Trial</td>
  <td>A</td>
  <td>B</td>
  <td>C</td>
 </tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td>-1</td>
  <td>-1</td>
  <td>-1</td>
 </tr>
 <tr>
  <td>2</td>
  <td>1</td>
  <td>-1</td>
  <td>-1</td>
 </tr>
 <tr>
  <td>3</td>
  <td>-1</td>
  <td>1</td>
  <td>-1</td>
 </tr>
 <tr>
  <td>4</td>
  <td>1</td>
  <td>1</td>
  <td>-1</td>
 </tr>
 <tr>
  <td>5</td>
  <td>-1</td>
  <td>-1</td>
  <td>1</td>
 </tr>
 <tr>
  <td>6</td>
  <td>1</td>
  <td>-1</td>
  <td>1</td>
 </tr>
 <tr>
  <td>7</td>
  <td>-1</td>
  <td>1</td>
  <td>1</td>
 </tr>
 <tr>
  <td>8</td>
  <td>1</td>
  <td>1</td>
  <td>1</td>
 </tr>
</tbody>
</table>
</div>
</center>

<p class="body">The response selected
for this experiment is the accuracy factor between the measured area and the
determined area (from pilot test). From the accuracy factor, the error <img src="001_files/image022.png"> can be
determined. The formulae are depicted in Eq. 5-7. The data is then tabulated
and analyzed using Minitab, a statistical software tool.</p>

<div class="equation">
<img src="001_files/image023.png" class="eqn">
<div class="eqn-number">(6)</div>
</div>

<div class="equation">
<img src="001_files/image024.png" class="eqn">
<div class="eqn-number">(7)</div>
</div>

<div class="equation">
<img src="001_files/image025.png" class="eqn">
<div class="eqn-number">(8)</div>
</div>

<h4>4. Results and Discussion</h4>
<p class="subhead">4. 1. Vehicle Detection</p>
<p class="body">Initial test results under normal and bright lighting show a lot
of false positives. It is observed that the algorithm could not segment the
desirable colors because the selected pre-set values of HSV could not
discriminate the desirable color range due to the increase in brightness. The
problem lies within the minimum and maximum range of V as it is dependent on
the lightness and darkness of image produced. Once the appropriate V values
range are set the system is tested for optimization.</p>

<p class="subhead">4. 2. Sensor Integration</p>
<p class="body">Once integrated with the system, it is run for 30 trials to get
the processing time each for 1 cycle of operation (vehicle detection to
decision making) [11]. Table 3
shows the descriptive statistics of the processing time.</p>

<center>
<p class="body">Table 3. Descriptive Statistics.</p>
<div class="widetable">
<table>
<thead>
 <tr>
  <td>Variable</td>
  <td>Mean</td>
  <td>Minimum</td>
  <td>Maximum</td>
 </tr>
</thead>
<tbody>
<tr>
  <td>Time(s)</td>
  <td>0.26025</td>
  <td>0.21077</td>
  <td>0.33039</td>
 </tr>
</tbody>
</table>
</div>
</center>

<p class="body">The mean processing time is observed to be 0.26s, with the
corresponding minimum and maximum time of 0.21s and 0.33s respectively. This is
considered satisfactory considering that the system is run under
resources-limited platform.</p>

<p class="subhead">4. 3. DOE Optimization Result</p>
<p class="body">The estimated effects and Coefficients for error table generated from
Minitab is shown in Table 3 (some values were omitted as the value of interest
is just the p-value). Error in this case is the error for the system to detect
the lead vehicle accurately. <img src="001_files/image026.png"> is
the probability that quantifies the strength of the evidence against the null
hypothesis in favor of the alternate hypothesis. Table 5 shows the <img src="001_files/image026.png"> and
its respective description.</p>

<center>
<p class="body">Table 4. Estimated Effects and
Coefficients for Error.</p>
<div class="widetable">
<table>
<thead>
<tr>
  <td>Term</td>
  <td>P</td>
 </tr>
</thead>
<tbody>
<tr>
  <td>Constant</td>
  <td>0.000</td>
 </tr>
 <tr>
  <td>A:
  Camera Angle</td>
  <td>0.000</td>
 </tr>
 <tr>
  <td>B:
  Illumination</td>
  <td>0.777</td>
 </tr>
 <tr>
  <td>C:
  Distance</td>
  <td>0.001</td>
 </tr>
 <tr>
  <td>A:
  Camera Angle*B: Illumination</td>
  <td>0.265</td>
 </tr>
 <tr>
  <td>A:
  Camera Angle*C: Distance</td>
  <td>0.001</td>
 </tr>
 <tr>
  <td>B:
  Illumination*C: Distance</td>
  <td>0.067</td>
 </tr>
 <tr>
  <td>A:
  Camera Angle*B: Illumination* C:
  Distance</td>
  <td>0.002</td>
 </tr>
</tbody>
</table>
</div>
</center>

<p class="body">From Table 4, it is
observed that Factor A, Factor C, Factor AC and Factor ABC have the least
p-value. Based on Table 5, it can be deduced that these factors have strong
evidence against the null hypothesis in favor of the alternate hypothesis. In
other words, these factors have highly significant effect to the contribution
of the error. Factor BC on the other hand, has weak evidence against the null
hypothesis in favor of alternate hypothesis, which means that the factor has
low effect to the contribution of error. Finally Factor B and Factor AB shows
no evidence against the null hypothesis in favor of the alternate hypothesis,
which means that they did not contribute to the error.</p>
<p class="body">In order to analyze
the best combination of factors with the least error, a cube plot is generated
as shown in Figure 8. From the cube plot it is observed that the error <img src="001_files/image027.png"> obtained is at
a satisfactory level. In exception of the (camera angle at 10°, bright
illumination and 0.5m distance) , (camera angle at 10° , bright illumination
and 1.0 m distance) and (camera angle at 10°, dim illumination and 1.0m
distance ) with error <img src="001_files/image027.png"> of 0.3,0.5 and 0.9 respectively the rest is
minimal. Therefore it can also be said that the best camera (elevation) angle
is at 20° regardless of the illumination and distance level <i
style='mso-bidi-font-style:normal'>(based from the tested condition)</i>.</p>

<center>
<p class="body">Table 5. Description
for the respective <img src="001_files/image028.png"> [11].</p>
<div class="widetable">
<table>
<thead>
<tr>
  <td>p-values</td>
  <td>Description</td>
 </tr>
</thead>
<tbody>
<tr>
  <td>p&gt;0.10</td>
  <td>No
  evidence against the null hypothesis in favour of the alternate hypothesis</td>
 </tr>
 <tr>
  <td>0.05&lt;p&lt;0.10</td>
  <td>Weak
  evidence against the null hypothesis in favour of alternate hypothesis</td>
 </tr>
 <tr>
  <td>0.01&lt;p&lt;0.05</td>
  <td>Moderate
  evidence against the null hypothesis in favour of alternate hypothesis</td>
 </tr>
 <tr>
  <td>p&lt;0.01</td>
  <td>Strong
  evidence against the null hypothesis in favour of alternate hypothesis</td>
 </tr>
</tbody>
</table>
</div>
</center>

<figure>
<img src="001_files/image9.png" class="article-img">

<figcaption>
Figure 9. Cube Plot Data Means for <img src="001_files/image030.png">.</figcaption>
</figure>

<h4>5. Conclusions</h4>
<p class="body">Based on the result of the 2<sup>3</sup> factorial experiments the
significant factors could be determined. It was concluded that the errors were
minimal. The experiment also showed that the system is robust towards different
illumination levels. However, there are also several limitations that need to
be addressed for future improvement. One of them is the possible illumination
glare due to very bright sunlight condition that may interfere with image
segmentation, and another is when the distance exceeds the tested values as the
system (and OpenCV libraries) may not have sufficient minimum data to detect
the contour area which may result in the failure of the system to detect the
lead vehicle totally.</p>

<h4>Acknowledgement</h4>
<p class="body">The authors
express their sincere thanks to the reviewers for their significant
contributions to the improvement of the final paper. This research was
supported by Development of Sustainable Platform for Land, Air and Naval
(SPALAN) System Grant. (Grant number: 1001/PELECT/814170).</p>

<h4 id="references">References</h4>
<div class="ref">
<p class="body">[1] N.
Abdul Rahman, &quot;Road Safety Situation In Malaysia,&quot; 2013. [Online].
Available:
http://www.unece.org/fileadmin/DAM/trans/doc/2013/wp1/newdelhi/Malaysia.pdf.
[Accessed 22 September 2014]. <a href="http://www.unece.org/fileadmin/DAM/trans/doc/2013/wp1/newdelhi/Malaysia.pdf." target="_blank">View Article</a></p>

<p class="body">[2] A.
Shacklock, X. Jian and W. Han, &quot;Visual Guidance for Autonomous Vehicles:
Capability and Challenges,&quot; in <i style='mso-bidi-font-style:normal'>Autonomous
Mobile Robots: Sensing, Control, Decision Making and Applications</i>, Florida,
CRC Press, 2006, pp. 6-40. <a href="https://www.crcpress.com/Autonomous-Mobile-Robots-Sensing-Control-Decision-Making-and-Applications/Ge/9780849337482" target="_blank">View Book</a></p>

<p class="body">[3] R.
L. Klaser, F. S. Osorio and D. F. Wolf, &quot;Simulation of an Autonomous
Vehicle with a Vision-Based Navigation System in Unstructured Terrains Using
OctoMap,&quot; in <i style='mso-bidi-font-style:normal'>2013 III Brazilian
Symposium on Computing Systems Engineering (SBESC)</i>, Niterói, 2013. <a href="http://dx.doi.org/10.1109/SBESC.2013.46" target="_blank">View Article</a></p>

<p class="body">[4] M.
Huang, R. Zhang, Y. Ma and Q. Yan, &quot;Research on Autonomous Driving Control
Method of Intelligent Vehicle Based on Vision Navigation,&quot; in <i
style='mso-bidi-font-style:normal'>2010 International Conference on
Computational Intelligence and Software Engineering</i>, Wuhan, 2010. <a href="http://dx.doi.org/10.1109/CISE.2010.5676770" target="_blank">View Article</a></p>

<p class="body">[5] K.
Sharma, K. Jeong and S. Kim, &quot;Vision based autonomous vehicle navigation
with self-organizing map feature matching technique,&quot; in <i
style='mso-bidi-font-style:normal'>11th International Conference on Control,
Automation and Systems (ICCAS)</i>, Gyeonggi-do, 2011. <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6106358&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6106358" target="_blank">View Article</a></p>

<p class="body">[6] Z.
Khalid, E.-A. Mohamed and M. Abdenbi, &quot;Stereo vision-based road obstacles
detection,&quot; in <i style='mso-bidi-font-style:normal'>8th International
Conference on Intelligent Systems: Theories and Applications (SITA)</i>, Rabat,
2013. <a href="http://dx.doi.org/10.1109/SITA.2013.6560817" target="_blank">View Article</a></p>

<p class="body">[7] R.
Gonzalez and R. Woods, Digital Image Processing, 3rd ed., Prentice Hall, 2007. <a href="http://www.academia.edu/7310640/Digital_Image_Processing_3rd_ed._-_R._Gonzalez_R._Woods" target="_blank">View Book</a></p>

<p class="body">[8] A.
Mordvintsev and A. K. Rahman, &quot;OpenCV-Python Tutorials
Documentation,&quot; 3 February 2014. [Online]. Available:
http://www.scribd.com/doc/218973725/Opencv-Python-Tutroals#scribd. <a href="http://www.scribd.com/doc/218973725/Opencv-Python-Tutroals#scribd" target="_blank">View Article</a></p>

<p class="body">[9] itseez,
&quot;About OpenCV,&quot; 2015. [Online]. Available:
http://opencv.org/about.html. [Accessed 18 October 2014]. <a href="http://opencv.org/about.html" target="_blank">View Article</a></p>

<p class="body">[10] G.
Bradski and A. Kaehler, Learning OpenCV, 1st ed., O'Reilly, 2008. <a href="http://shop.oreilly.com/product/9780596516130.do" target="_blank">View Book</a></p>

<p class="body">[11] D.
C. Montgomery, Design and Analysis of Experiments International Student
Version, 8th ed., Singapore: John Wiley &amp; Sons, 2013. <a href="http://ca.wiley.com/WileyCDA/WileyTitle/productCd-1118097939.html" target="_blank">View Book</a></p>
</div> <!--REF -->
  </div> <!--INDENT -->

  </div> <!--Main Content -->
</div>
</div>

  <footer>
<div class="grid">
  <div class="unit unit-s-1 unit-s-1-3 unit-m-1-3 unit-l-1-3">
    <div class="unit-spacer">
      <ul class="footer-links">
        <li><a href="http://avestia.com/" class="body-link">Avestia Publishing</a></li>
        <li><a href="http://avestia.com/journals" class="body-link">Journals</a></li>
        <li><script>var refURL = window.location.protocol + "//" + window.location.host + window.location.pathname; document.write('<a href="http://international-aset.com/feedback/?refURL=' + refURL+'">Feedback</a>');</script></li>
        <li><a href="http://avestia.com/terms" class="body-link">Terms of Use</a></li>
        <li><a href="../sitemap" class="body-link">Sitemap</a></li>
      </ul>
    </div>
  </div>

  <div class="unit unit-s-1 unit-s-1-3 unit-m-1-3 unit-l-1-3">
    <div class="unit-spacer">
      <p class="body">
        Avestia Publishing,<br>
        International ASET Inc.<br>
        Unit 417, 1376 Bank St.<br>
        Ottawa, ON, Canada, K1H 7Y3<br>
        +1 613-695-3040<br>
        <a href="mailto:info@avestia.com" class="body-link">info@avestia.com</a>
      </p>
    </div>
  </div>

  <div class="unit unit-s-1 unit-s-1-3 unit-m-1-3 unit-l-1-3">
    <div class="unit-spacer social">
    <form class="subscribe" action="../register.php" method="post">
            <span id="sprytextfield2"><input name="email" type="text" id="email" value="Join our mailing list"
              onblur="if (this.value == '') {this.value = 'Join our mailing list';}"
        onfocus="if (this.value == 'Join our mailing list') {this.value = '';}" ></span>
      <input type="submit" name="submit" value="Submit" class="form_button" />
        </form>
        
      <div class="unit unit-s-1-1 unit-m-1-1 unit-l-1-1">
        <a href="https://www.facebook.com/pages/International-Academy-of-Science-Engineering-and-Technology/207827708283" target="blank" title="International ASET Inc. Facebook Page">
          <img src="../img/fb.png" border="0" onmouseover="this.src='../img/fb-hover.png'" onmouseout="this.src='../img/fb.png'">
        </a>
      </div>

      <div class="unit unit-s-1-1 unit-m-1-1 unit-l-1-1">
        <a href="https://twitter.com/ASET_INC" target="blank" title="International ASET Inc. Twitter">
          <img src="../img/twitter.png" border="0" onmouseover="this.src='../img/twitter-hover.png'" onmouseout="this.src='../img/twitter.png'">
        </a>
      </div>

      <div class="unit unit-s-1-1 unit-m-1-1 unit-l-1-1">
        <a href="https://www.linkedin.com/company/1169039" target="blank" title="International ASET Inc. LinkedIn">
          <img src="../img/linkedin.png" border="0" onmouseover="this.src='../img/linkedin-hover.png'" onmouseout="this.src='../img/linkedin.png'">
        </a>
      </div>

      <div class="unit unit-s-1-1 unit-m-1-1 unit-l-1-1">
        <a href="https://plus.google.com/u/0/+International-aset/posts" target="blank" title="International ASET Inc. Google+ Page">
          <img src="../img/google.png" border="0" onmouseover="this.src='../img/google-hover.png'" onmouseout="this.src='../img/google.png'">
        </a>
      </div>

      <p class="body">© Copyright 2015, International ASET Inc. – All Rights Reserved.</p>
    </div>
  </div>

</div>
</footer>
</div>

 <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
    <script src="../js/cbpAnimatedHeader.min.js"></script>
    <script src="../js/SpryValidationSelect.js" type="text/javascript"></script>

    <script src="../js/SpryValidationTextField.js" type="text/javascript"></script>

    <script src="../js/SpryValidationConfirm.js" type="text/javascript"></script>

    <script src="../js/SpryValidationCheckbox.js" type="text/javascript"></script>
    <script src="../js/SpryValidationTextarea.js" type="text/javascript"></script>

<script src="../js/classie.js"></script>
<script src="../js/jquery.easing.js"></script>
<script src="../js/jquery.mousewheel.js"></script>
<script defer src="../js/demo.js"></script>
<script type="text/javascript" src="../css/animate.min.css"></script>
<script type="text/javascript" src="../js/jnav.js"></script>

<script type="text/javascript">
<!--
var sprytextfield1 = new Spry.Widget.ValidationTextField("sprytextfield1", "none");
var sprytextfield2 = new Spry.Widget.ValidationTextField("sprytextfield2", "email");
var sprytextfield3 = new Spry.Widget.ValidationTextField("sprytextfield3");
var sprytextfield4 = new Spry.Widget.ValidationTextField("sprytextfield4");
var spryselect2 = new Spry.Widget.ValidationSelect("spryselect2", {invalidValue:"-1"});
var sprytextarea1 = new Spry.Widget.ValidationTextarea("sprytextarea1");
var sprytextfield5 = new Spry.Widget.ValidationTextField("sprytextfield5");
var sprytextfield6 = new Spry.Widget.ValidationTextField("sprytextfield6");
//-->
</script>

    <script type="text/javascript">
/*
  Slidemenu
*/
(function() {
  var $body = document.body
  , $menu_trigger = $body.getElementsByClassName('menu-trigger')[0];

  if ( typeof $menu_trigger !== 'undefined' ) {
    $menu_trigger.addEventListener('click', function() {
      $body.className = ( $body.className == 'menu-active' )? '' : 'menu-active';
    });
  }

}).call(this);
</script>

<script type="text/javascript">
/*
  Slidemenu
*/
(function() {
  var $body = document.body
  , $menu_trigger = $body.getElementsByClassName('menu-trigger-1')[0];

  if ( typeof $menu_trigger !== 'undefined' ) {
    $menu_trigger.addEventListener('click', function() {
      $body.className = ( $body.className == 'menu-active' )? '' : 'menu-active';
    });
  }

}).call(this);
</script>
</body>
</html>